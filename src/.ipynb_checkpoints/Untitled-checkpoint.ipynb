{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import os\n",
    "\n",
    "noad2wn = {}\n",
    "\n",
    "excluding_list = {'be', 'do'}\n",
    "\n",
    "def load_sense_map(map_filename = '../data/manual_map.txt'):\n",
    "    f = open(map_filename, 'r')\n",
    "    for line in f.readlines():\n",
    "        # split into a noad sense and a list of wordnet senses\n",
    "        noad, wn = line.strip().split()\n",
    "        # list of wordnet senses\n",
    "#        wns = wn.split(\",\")\n",
    "        noad2wn[noad] = wn\n",
    "    f.close()\n",
    "    \n",
    "def output_break(break_str):\n",
    "    if break_str == \"SPACE_BREAK\":\n",
    "        return \" \"\n",
    "    elif break_str == \"SENTENCE_BREAK\":\n",
    "        return \"\\n\"\n",
    "    elif break_str == \"NO_BREAK\":\n",
    "        return \" \"\n",
    "    else:  # \"LINE_BREAK\"\n",
    "        return \"\\n\"\n",
    "    \n",
    "def transform_corpus(in_filename, out_filename):\n",
    "    \n",
    "    tree = ET.parse(in_filename)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    out_f = open(out_filename, 'w')\n",
    "    \n",
    "    hot_positions = []\n",
    "    sentence = []\n",
    "    word_index = 0\n",
    "    \n",
    "    for child in root:\n",
    "        a = child.attrib\n",
    "        \n",
    "        # before going to the the next sentence\n",
    "        if output_break(a['break_level']) == '\\n':\n",
    "            \n",
    "            # if there is at least one ambiguous location\n",
    "            if len(hot_positions) != 0:\n",
    "                \n",
    "                # output the sentence first\n",
    "                for w in sentence:\n",
    "                    if w != '\\n':\n",
    "                        out_f.write(w.encode('utf-8'))\n",
    "                # output a list of senses for the ambiguous words in the current sentence\n",
    "                out_f.write('\\n'  + str(len(hot_positions)))\n",
    "                for t in hot_positions:\n",
    "                    out_f.write(\"\\n#\" + str(t[0]) + \" \")\n",
    "                    out_f.write(t[1].encode('utf-8') + \" \")\n",
    "                    out_f.write(t[2].encode('utf-8') + \" \")\n",
    "                    out_f.write(t[3].encode('utf-8'))\n",
    "                out_f.write(\"\\n\")\n",
    "\n",
    "            # clean up\n",
    "            word_index = 0\n",
    "            hot_positions = []\n",
    "            sentence = []\n",
    "        \n",
    "        sentence.append(output_break(a['break_level']))\n",
    "        sentence.append(a['text'])\n",
    "        \n",
    "        if 'sense' in a \\\n",
    "            and a['lemma'] not in excluding_list \\\n",
    "            and a['sense'] in noad2wn:\n",
    "            hot_positions.append((word_index, a['lemma'], a['pos'], noad2wn[a['sense']]))\n",
    "            \n",
    "        word_index += 1\n",
    "        \n",
    "    # handle the last list of hot positions\n",
    "    # if there is at least one ambiguous location\n",
    "    if len(hot_positions) != 0:\n",
    "        # output the sentence first\n",
    "        for w in sentence:\n",
    "            if w != '\\n':\n",
    "                out_f.write(w.encode('utf-8'))\n",
    "        # output a list of senses for the ambiguous words in the current sentence\n",
    "        out_f.write('\\n'  + str(len(hot_positions)))\n",
    "        for t in hot_positions:\n",
    "            out_f.write(\"\\n#\" + str(t[0]) + \" \")\n",
    "            out_f.write(t[1].encode('utf-8') + \" \")\n",
    "            out_f.write(t[2].encode('utf-8') + \" \")\n",
    "            out_f.write(t[3].encode('utf-8'))\n",
    "        out_f.write('\\n')\n",
    "    \n",
    "    out_f.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # load two mappings from NOAD senses to WN senses\n",
    "    load_sense_map('../data/manual_map.txt')\n",
    "    load_sense_map('../data/algorithmic_map.txt')\n",
    "    \n",
    "    # go through folder semcor\n",
    "    for filename in os.listdir('../data/semcor/'):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            print(filename)\n",
    "            items = filename.split('.')\n",
    "            transform_corpus('../data/semcor/' + filename, '../data/semcor_txt/' + items[0] + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
